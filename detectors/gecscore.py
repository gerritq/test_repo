# Code taken from: https://github.com/NLP2CT/GECScore/blob/main/detector/GECScore.py
# Some mods include: creation of class, change data loading -  compare process_data

import random
import numpy as np
import torch
import tqdm
import argparse
import json
import os
from rouge import Rouge
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils import load_jsonl

rouge = Rouge()


class GECScore:

    def __init__(self,
                lang,
                model="gpt-3.5-turbo"):
        self.model = model
        self.lang = lang
        self.prompts = {
                        "en": "Correct the grammar errors in the following text: {text}\nCorrected text:",
                        "pt": "Corrija os erros gramaticais no texto a seguir: {text}\nTexto corrigido:",
                        "vi": "Sửa lỗi ngữ pháp trong văn bản sau: {text}\nVăn bản đã sửa:"
                        }
        self.api_key = os.environ.get("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key)

    def _chat(self, prompt):
        """
        Interacts with GPT-4o or another LLM to generate text.

        Args:
            prompt (str): Input text to prompt the LLM.
            model (str): The LLM model to use.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt},
                ],
                temperature=0.01,
            )
            return completion.choices[0].message.content
        except Exception as e:
            print(e)

    def process_data(self, texts):
        
        # Threading: use worker function for API calls
        results = {}
        
        def process_text(idx, text):
            prompt = self.prompts[self.lang].format(text=text)
            gec_text = self._chat(prompt)
            return idx, text, gec_text
        
        with ThreadPoolExecutor(max_workers=6) as executor:
            future_to_idx = {executor.submit(process_text, idx, text): idx for idx, text in enumerate(texts)}
            
            for future in tqdm.tqdm(as_completed(future_to_idx), total=len(texts)):
                idx, text, gec_text = future.result()
                results[idx] = (text, gec_text)
        
        predictions = []
        for idx in range(len(texts)):
            text, gec_text = results[idx]
            try:
                rouge_score = rouge.get_scores(text, gec_text, avg=True)
                llm_text_rouge2_score = rouge_score['rouge-2']['f']
            except Exception as rouge_error:
                print(f"Rouge error for index {idx}: {rouge_error}")
                llm_text_rouge2_score = 0.0
                
            predictions.append(llm_text_rouge2_score)
            
        return predictions